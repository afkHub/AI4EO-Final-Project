{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 12068,
     "status": "ok",
     "timestamp": 1724252002933,
     "user": {
      "displayName": "Fırat ESPACE",
      "userId": "14229422914829834804"
     },
     "user_tz": -120
    },
    "id": "UZ6VaragUPrm"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import tifffile as tiff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SI-STSAR-7 Dataset\n",
    "class SARImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir #initializing dataset by reading the root directory\n",
    "        self.transform = transform #the transformations to be applied to the images\n",
    "        self.classes = sorted(os.listdir(root_dir))  #sorts classes\n",
    "        self.samples = [] #lists for corresponding labels\n",
    "        #loop for each class folder in the root directory\n",
    "        for label, class_dir in enumerate(self.classes):\n",
    "            class_path = os.path.join(root_dir, class_dir)\n",
    "            #loop for each image in the class folder\n",
    "            for fname in os.listdir(class_path):\n",
    "                if fname.endswith(('tif')):\n",
    "                    self.samples.append((os.path.join(class_path, fname), label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples) #returns the total number of images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx] #image and label at the specified index\n",
    "        img = tiff.imread(img_path) #load tif image\n",
    "        img = Image.fromarray(img) #convert tif image to PIL image\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img) #apply the specified transformations to the image\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 234787,
     "status": "ok",
     "timestamp": 1724252237717,
     "user": {
      "displayName": "Fırat ESPACE",
      "userId": "14229422914829834804"
     },
     "user_tz": -120
    },
    "id": "my1vH8iYAMlX",
    "outputId": "347a75f1-ee02-4298-85ae-1982f4e02f3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples in the full dataset: 164564\n",
      "Number of samples in the training set: 98738\n",
      "Number of samples in the testing set: 32914\n",
      "Class 0: GI_6s - GWI_6s - NI_6s\n",
      "Class 1: MediumFI_6s\n",
      "Class 2: OW_6s\n",
      "Class 3: ThickFI_6s\n",
      "Class 4: ThinFI_6s\n"
     ]
    }
   ],
   "source": [
    "# dataset direction\n",
    "data_dir = 'only1img4allHHc5 - Copy'\n",
    "\n",
    "# Data augmentation\n",
    "data_augmentation = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    #transforms.RandomRotation(10)\n",
    "])\n",
    "\n",
    "# Preprocessing\n",
    "preprocess_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Applying data augmentation and preprocessing to only training dataset\n",
    "train_transform = transforms.Compose([\n",
    "    data_augmentation,\n",
    "    preprocess_transform\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "full_dataset = SARImageDataset(root_dir=data_dir, transform=preprocess_transform)\n",
    "\n",
    "\n",
    "# Perform a 70-15-15 train-val-test split on the original dataset length\n",
    "dataset_size = len(full_dataset)\n",
    "train_size = int(0.6 * dataset_size)\n",
    "val_size = int(0.2 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size, test_size]) # split the datasets\n",
    "\n",
    "#data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "# Print the sizes of the datasets\n",
    "print(f\"Total number of samples in the full dataset: {len(full_dataset)}\")\n",
    "print(f\"Number of samples in the training set: {len(train_dataset)}\")\n",
    "print(f\"Number of samples in the testing set: {len(test_dataset)}\")\n",
    "\n",
    "# print the classes\n",
    "def print_class(root_dir):\n",
    "    class_names = sorted(os.listdir(root_dir)) #sorts and lists all folders in the main folder\n",
    "    for i, class_name in enumerate(class_names): #prints each class with folder names\n",
    "        print(f\"Class {i}: {class_name}\")\n",
    "print_class(data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1724252237718,
     "user": {
      "displayName": "Fırat ESPACE",
      "userId": "14229422914829834804"
     },
     "user_tz": -120
    },
    "id": "ZZKHmypEVCgq"
   },
   "outputs": [],
   "source": [
    "# Training, Validation, and Testing\n",
    "from tqdm import tqdm #for time counting\n",
    "\n",
    "def train(train_loader, val_loader, model, criterion, optimizer, num_epochs, device):\n",
    "    model.to(device)\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() #set model for training mode\n",
    "        running_loss = 0.0 #initializing running loss\n",
    "        correct = 0 #for time counting\n",
    "        total = 0 #for time counting\n",
    "        #progress bar for time counting\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for i, (images, labels) in progress_bar:\n",
    "            images, labels = images.to(device), labels.to(device)  #data move to the appropriate device/GPU\n",
    "            optimizer.zero_grad() #all gradients to zero\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels) #calculate loss\n",
    "            loss.backward() #computing the gradient of loss with respect to each parameters in model\n",
    "            optimizer.step() #updating model\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0) #accumulating loss\n",
    "            \n",
    "            #time counting\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            progress_bar.set_postfix(loss=running_loss/(i+1), accuracy=100.*correct/total)\n",
    "        epoch_accuracy = 100. * correct / total #accuracy per epoch\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset) #avg loss computation\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.2f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "        #validate the model\n",
    "        val_accuracy = validate(val_loader, model, device)\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_model = model.state_dict()\n",
    "\n",
    "    model.load_state_dict(best_model)  #load the best model\n",
    "    print(\"Training complete\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def test(test_loader, model, device):\n",
    "    model.to(device)\n",
    "    model.eval() #set evaluation mode\n",
    "    correct = 0 #correct predictions\n",
    "    total = 0 #total samples\n",
    "    \n",
    "    with torch.no_grad():  #iterating over the test dataset without tracking gradients\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)  #move data to the appropriate device\n",
    "            \n",
    "            outputs = model(images)  #forward pass\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)  #get the index of the max log-probability (prediction)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    accuracy = correct / total\n",
    "    print(f'Test Accuracy %: {accuracy * 100:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def validate(val_loader, model, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f'Validation Accuracy %: {accuracy * 100:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "#define the device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #for \"RuntimeError: Found no NVIDIA driver on your system.\" error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\co1st\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\co1st\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#trying ResNet model\n",
    "\n",
    "# Load the pre-trained ResNet18 model\n",
    "model = resnet18(pretrained=True) #model weights pre-trained on the ImageNet dataset, \"True\" gave better results than \"False\", about 1-2%\n",
    "\n",
    "# Redefine first layer (single-band input)\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) #RuntimeError: Given groups=1, weight of size [64, 3, 7, 7], expected input[32, 1, 32, 32] to have 3 channels, but got 1 channels instead\n",
    "#the data does not have 3 channel, it's single band HH. so first value should be \"1\"\n",
    "\n",
    "# Disable gradients from all layers of the model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# replace the last layer with a new linear layer where gradients are active\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 5) #5 classes\n",
    "\n",
    "# enable gradients for the new layer\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(model.fc.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "executionInfo": {
     "elapsed": 941336,
     "status": "error",
     "timestamp": 1724253179049,
     "user": {
      "displayName": "Fırat ESPACE",
      "userId": "14229422914829834804"
     },
     "user_tz": -120
    },
    "id": "eyFjznxtqGGX",
    "outputId": "febb4f2e-c0d2-4fe4-df41-b766d947ca8e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|████████████████████████████████████████| 3086/3086 [04:05<00:00, 12.55it/s, accuracy=57.6, loss=33.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.04, Accuracy: 57.63%\n",
      "Validation Accuracy %: 60.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|████████████████████████████████████████| 3086/3086 [04:15<00:00, 12.07it/s, accuracy=59.4, loss=31.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.99, Accuracy: 59.44%\n",
      "Validation Accuracy %: 60.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|████████████████████████████████████████| 3086/3086 [04:14<00:00, 12.12it/s, accuracy=59.9, loss=31.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.98, Accuracy: 59.93%\n",
      "Validation Accuracy %: 61.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|████████████████████████████████████████| 3086/3086 [03:58<00:00, 12.95it/s, accuracy=60.6, loss=30.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.96, Accuracy: 60.59%\n",
      "Validation Accuracy %: 61.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|████████████████████████████████████████| 3086/3086 [10:21<00:00,  4.96it/s, accuracy=60.9, loss=30.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.95, Accuracy: 60.95%\n",
      "Validation Accuracy %: 59.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████████████████████████████████████| 3086/3086 [04:04<00:00, 12.62it/s, accuracy=61.4, loss=30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.94, Accuracy: 61.43%\n",
      "Validation Accuracy %: 54.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|████████████████████████████████████████| 3086/3086 [03:53<00:00, 13.21it/s, accuracy=61.7, loss=29.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.93, Accuracy: 61.69%\n",
      "Validation Accuracy %: 62.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|████████████████████████████████████████| 3086/3086 [03:54<00:00, 13.16it/s, accuracy=62.1, loss=29.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.92, Accuracy: 62.14%\n",
      "Validation Accuracy %: 60.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|████████████████████████████████████████| 3086/3086 [03:52<00:00, 13.27it/s, accuracy=62.6, loss=29.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.91, Accuracy: 62.55%\n",
      "Validation Accuracy %: 56.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|███████████████████████████████████████| 3086/3086 [04:12<00:00, 12.22it/s, accuracy=62.9, loss=28.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.90, Accuracy: 62.90%\n",
      "Validation Accuracy %: 52.90%\n",
      "Training complete\n",
      "Test Accuracy %: 52.79%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "# turning on gradient calculation for all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "#the optimizer to optimize all parameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# train the model for fine-tuning\n",
    "fine_tuned_model = train(train_loader, val_loader, model, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "# test the model for fine-tuning\n",
    "fine_tuned_accuracy = test(test_loader, fine_tuned_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When low accuracy was seen with ResNet, which gave the fastest results, no time was wasted by continuing with EfficientNet. It is obvious that it will give similar low results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1fhTdupNOSVR80VLc2zKryCM5F2Ui7Mxs",
     "timestamp": 1723748616443
    },
    {
     "file_id": "1pPJznDAHHcSUoa84oGDXOH9RpTJa5ke8",
     "timestamp": 1723727930619
    },
    {
     "file_id": "1fhzMBET3NOl3519ue10lXB65CBggrc0d",
     "timestamp": 1723559500991
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
